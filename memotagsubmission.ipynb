{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QdF_qZQdXpAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEbl55EKXedD"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/snakers4/silero-vad.git\n",
        "!pip install torch librosa pandas scikit-learn pydub\n",
        "!pip install git+https://github.com/openai/whisper.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import whisper\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pydub import AudioSegment\n",
        "from sklearn.cluster import KMeans\n",
        "from vad import VoiceActivityDetector\n",
        "\n",
        "# Load models\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "# Silero VAD model\n",
        "torch.set_num_threads(1)\n",
        "vad_model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', trust_repo=True)\n",
        "(get_speech_timestamps, _, read_audio, _, _) = utils\n",
        "\n",
        "# Paths\n",
        "BASE_DIR = \"/content/drive/MyDrive/icaasp_paper/dementiabank\"\n",
        "\n",
        "# Get first 15 .mp3 files\n",
        "def get_mp3_files(base_dir, limit=15):\n",
        "    mp3_files = []\n",
        "    for root, _, files in os.walk(base_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".mp3\"):\n",
        "                mp3_files.append(os.path.join(root, file))\n",
        "            if len(mp3_files) >= limit:\n",
        "                return mp3_files\n",
        "    return mp3_files\n",
        "\n",
        "# Convert mp3 to wav (for analysis)\n",
        "def convert_mp3_to_wav(mp3_path):\n",
        "    audio = AudioSegment.from_mp3(mp3_path)\n",
        "    wav_path = mp3_path.replace(\".mp3\", \".wav\")\n",
        "    audio.export(wav_path, format=\"wav\")\n",
        "    return wav_path\n",
        "\n",
        "# Extract features\n",
        "def extract_audio_features(wav_path):\n",
        "    y, sr = librosa.load(wav_path)\n",
        "    duration = librosa.get_duration(y=y, sr=sr)\n",
        "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "    pitch = np.mean(librosa.yin(y, fmin=50, fmax=300))\n",
        "    return y, sr, duration, tempo, pitch\n",
        "\n",
        "# Count pauses using Silero VAD\n",
        "def count_pauses(wav_path):\n",
        "    wav = read_audio(wav_path, sampling_rate=16000)\n",
        "    speech_timestamps = get_speech_timestamps(wav, vad_model, sampling_rate=16000)\n",
        "\n",
        "    # Pauses = gaps between speech segments\n",
        "    pauses = 0\n",
        "    for i in range(1, len(speech_timestamps)):\n",
        "        prev_end = speech_timestamps[i - 1]['end']\n",
        "        curr_start = speech_timestamps[i]['start']\n",
        "        gap = (curr_start - prev_end) / 16000  # convert to seconds\n",
        "        if gap > 0.3:\n",
        "            pauses += 1\n",
        "    return pauses\n",
        "\n",
        "# Transcribe using Whisper\n",
        "def transcribe_whisper(audio_path):\n",
        "    result = whisper_model.transcribe(audio_path)\n",
        "    return result['text']\n",
        "\n",
        "# Run pipeline\n",
        "audio_data = []\n",
        "for mp3_path in get_mp3_files(BASE_DIR):\n",
        "    try:\n",
        "        wav_path = convert_mp3_to_wav(mp3_path)\n",
        "        y, sr, duration, tempo, pitch = extract_audio_features(wav_path)\n",
        "        pauses = count_pauses(wav_path)\n",
        "        transcript = transcribe_whisper(wav_path)\n",
        "        speech_rate = len(transcript.split()) / duration if duration > 0 else 0\n",
        "\n",
        "        audio_data.append({\n",
        "            \"file\": mp3_path,\n",
        "            \"duration\": duration,\n",
        "            \"speech_rate\": speech_rate,\n",
        "            \"tempo\": tempo,\n",
        "            \"pitch\": pitch,\n",
        "            \"pauses\": pauses,\n",
        "            \"transcript\": transcript\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {mp3_path}: {e}\")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(audio_data)\n",
        "\n",
        "# Cluster\n",
        "features = df[[\"speech_rate\", \"tempo\", \"pitch\", \"pauses\"]]\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(features)\n",
        "df[\"cluster\"] = kmeans.labels_\n",
        "\n",
        "# Show result\n",
        "print(df[[\"file\", \"speech_rate\", \"tempo\", \"pitch\", \"pauses\", \"cluster\"]])\n"
      ],
      "metadata": {
        "id": "kKcqS1Y7XfyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Derive true label from folder name (basic example)\n",
        "def extract_true_label(file_path):\n",
        "    parts = file_path.lower().split('/')\n",
        "    for part in parts:\n",
        "        if \"dementia\" in part:\n",
        "            return \"dementia\"\n",
        "        elif \"mci\" in part:\n",
        "            return \"mci\"\n",
        "        elif \"hc\" in part:\n",
        "            return \"hc\"\n",
        "    return \"unknown\"\n",
        "\n",
        "df[\"true_label\"] = df[\"file\"].apply(extract_true_label)\n",
        "\n",
        "# Map true labels to integers for comparison\n",
        "label_map = {label: idx for idx, label in enumerate(df[\"true_label\"].unique())}\n",
        "df[\"true_label_id\"] = df[\"true_label\"].map(label_map)\n",
        "df[\"predicted_label\"] = df[\"cluster\"]\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nðŸ“Š Classification Report (Unsupervised Cluster vs. True Labels):\")\n",
        "print(classification_report(df[\"true_label_id\"], df[\"predicted_label\"], target_names=label_map.keys()))\n"
      ],
      "metadata": {
        "id": "peykw2_UX52v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}